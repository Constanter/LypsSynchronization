# Sync
Для тестирования скрипта:
1)запустить ноутбук Wav2Lip.ipynb
2)Выполнить все пунткы(в одном месте прописать "y")
3)В ноутбук уже загружен тестовый пример видео и аудио,в низком качестве
4)Для добавления своих видео и аудио необходимо в следующей команде прописать путь для них:
!cd Sync && python inference.py --checkpoint_path checkpoints/wav2lip_gan.pth --face "<video_path>" --audio "<audio_path>" 
5)Результат сохраняется в results/result_voice.mp4.

Было сделано:
1)Произведен поиск среди работ на тему LipsSynch среди SOTA решений на  paperswithcode.
2)Была выбрана работа http://cdn.iiit.ac.in/cdn/cvit.iiit.ac.in/images/Projects/Speech-to-Lip/paper.pdf
3)Был найден репозиторий с реализацией https://github.com/Rudrabha/Wav2Lip
4)Изначально вес модели был 435мб.Так как нас интересовал только инференс модели.я выделив только state_dict модели получил вес в 145мб
5)Изменил инференс модели.
6)Собрал всё в один репозиторий,чтобы можно было быстро протестировать,и не подгружать дополнительно googledrive для загрузки весов и данных.

Возможности для улучшения(для серверного и детскопного инференса(места где есть возможность работы на gpu)можно использовать текущую модель): 
1)была произведено  посттренировочное статическое квантование модели,и ее вес сократился до 36мб,
но необходимо было больше времени для того чтобы немного изменить саму модель.
В дальнейшем выполнив это пункт можно как уменьшить вес модели так и увеличить ее скорость ее работы.
2)можно произвести оптимизацию модели для мобильных устройств,встроенными методами PyTorch.
3)Можно перенести в ONNX
4)Также после оптимизации можно попробовать добавить сеть,которая  будет улучшать качество видео на выходе 
